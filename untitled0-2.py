# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y4t1y4Cqws2B7udBzRyq6Jzd8FDMi1bv
"""

from datasets import Dataset, load_dataset, load_metric
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline
import os
import json

train_file_path = 'redditComments_train.jsonlist'
test_file_path = 'redditComments_test_notGraded.jsonlist'

def json_to_pandas(path):
  dataset = []
  with open(path) as F:
    for l in F.readlines():
      l = json.loads(l)
      newL = dict()
      i = l['subreddit']
      newL['labels'] = l['subreddit']
      newL['text'] = l['body']
      newL['labels'] = 0 if i == 'MarioMaker' else 1 if i == 'Nerf' else 2 if i == 'ukelele' else 3
      dataset.append(newL)
  dataset = pd.DataFrame.from_dict(dataset)
  print(dataset)
  return Dataset.from_pandas(dataset)

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
def tokenize_function(examples):
	# print(examples['subreddit'])
	return tokenizer(examples["text"], padding="max_length", truncation=True)


model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels = 4)
training_args = TrainingArguments(
	per_device_train_batch_size = 8,
	per_device_eval_batch_size = 8,
	output_dir="/content/drive/My Drive/NLP HW4 Data/model out/", 
	evaluation_strategy="steps", 
	num_train_epochs = 1,
	save_steps = 1500,
	learning_rate = 5e-6, #defaults to 5e-5
	gradient_accumulation_steps = 1,
	load_best_model_at_end = True
)

metric = load_metric("accuracy")
def compute_metrics(eval_pred):
  logits,labels = eval_pred
  predictions = np.argmax(logits, axis=-1)

  result = metric.compute(predictions=predictions, references=labels)
  return result

global train_dataset
global eval_dataset

def classifySubreddit_train(trainFile):
  dataset_train = json_to_pandas(trainFile)
  tokenized_dataset_train = dataset_train.map(tokenize_function, batched=True)
  train_testvalid = tokenized_dataset_train.train_test_split(test_size=0.1)
  trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = train_testvalid['train'],
    eval_dataset = train_testvalid['test'],
    tokenizer = tokenizer,
    compute_metrics = compute_metrics
  )
  train_dataset = train_testvalid['train']
  eval_dataset = train_testvalid['test']
  trainer.train()


def classifySubreddit_test(testFile):
  dataset_test = json_to_pandas(testFile)
  tokenized_dataset_test = dataset_test.map(tokenize_function, batched=True)
  output = trainer.predict(test_dataset = tokenized_dataset_test)
  outputArray = []

  for label in output.predictions: 
    m = label.argmax()
    m = 'MarioMaker' if m == 0 else 'Nerf' if m == 1 else 'ukelele' if m == 2 else 'newToTheNavy'
    outputArray.append(m)

  return outputArray

classifySubreddit_train(train_file_path)

print(classifySubreddit_test(test_file_path))